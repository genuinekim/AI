{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200b3fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d59a1a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 197463\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "      <th>cc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>28898</th>\n",
       "      <td>Get me out of here.</td>\n",
       "      <td>Sortez-moi de là !</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94694</th>\n",
       "      <td>I didn't know what I'd find.</td>\n",
       "      <td>J'ignorais ce que je trouverais.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11455</th>\n",
       "      <td>Make it simple.</td>\n",
       "      <td>Simplifie.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185925</th>\n",
       "      <td>I really don't understand what you're talking ...</td>\n",
       "      <td>Je ne comprends vraiment pas de quoi vous parlez.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103414</th>\n",
       "      <td>I want to get back to Boston.</td>\n",
       "      <td>Je veux retourner à Boston.</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      eng  \\\n",
       "28898                                 Get me out of here.   \n",
       "94694                        I didn't know what I'd find.   \n",
       "11455                                     Make it simple.   \n",
       "185925  I really don't understand what you're talking ...   \n",
       "103414                      I want to get back to Boston.   \n",
       "\n",
       "                                                      fra  \\\n",
       "28898                                  Sortez-moi de là !   \n",
       "94694                    J'ignorais ce que je trouverais.   \n",
       "11455                                          Simplifie.   \n",
       "185925  Je ne comprends vraiment pas de quoi vous parlez.   \n",
       "103414                        Je veux retourner à Boston.   \n",
       "\n",
       "                                                       cc  \n",
       "28898   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "94694   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "11455   CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "185925  CC-BY 2.0 (France) Attribution: tatoeba.org #2...  \n",
       "103414  CC-BY 2.0 (France) Attribution: tatoeba.org #5...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "file_path = os.getenv('HOME') + '/aiffel/translator_seq2seq/data/fra.txt'\n",
    "lines = pd.read_csv(file_path, names=['eng', 'fra', 'cc'], sep='\\t')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5) #샘플 5개 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0be3f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>38275</th>\n",
       "      <td>That's why Tom quit.</td>\n",
       "      <td>C'est pour ça que Tom a démissionné.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49338</th>\n",
       "      <td>I did thirty push-ups.</td>\n",
       "      <td>J'ai fais trente pompes.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35173</th>\n",
       "      <td>He has a nose bleed.</td>\n",
       "      <td>Il saigne du nez.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4792</th>\n",
       "      <td>Help us, Tom.</td>\n",
       "      <td>Aidez-nous, Tom.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21165</th>\n",
       "      <td>Tom kept texting.</td>\n",
       "      <td>Tom a continué à envoyer des textos.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          eng                                   fra\n",
       "38275    That's why Tom quit.  C'est pour ça que Tom a démissionné.\n",
       "49338  I did thirty push-ups.              J'ai fais trente pompes.\n",
       "35173    He has a nose bleed.                     Il saigne du nez.\n",
       "4792            Help us, Tom.                      Aidez-nous, Tom.\n",
       "21165       Tom kept texting.  Tom a continué à envoyer des textos."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = lines[['eng', 'fra']][:50000]\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "585b5474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng</th>\n",
       "      <th>fra</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9491</th>\n",
       "      <td>You're crafty.</td>\n",
       "      <td>\\t Vous êtes rusés. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35449</th>\n",
       "      <td>His bicycle is blue.</td>\n",
       "      <td>\\t Sa bicyclette est bleue. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25940</th>\n",
       "      <td>That wasn't a lie.</td>\n",
       "      <td>\\t Ce n'était pas un mensonge. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4154</th>\n",
       "      <td>We like him.</td>\n",
       "      <td>\\t Nous l'aimons bien. \\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15953</th>\n",
       "      <td>They're awesome.</td>\n",
       "      <td>\\t Ils sont géniaux. \\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        eng                                fra\n",
       "9491         You're crafty.             \\t Vous êtes rusés. \\n\n",
       "35449  His bicycle is blue.     \\t Sa bicyclette est bleue. \\n\n",
       "25940    That wasn't a lie.  \\t Ce n'était pas un mensonge. \\n\n",
       "4154           We like him.          \\t Nous l'aimons bien. \\n\n",
       "15953      They're awesome.            \\t Ils sont géniaux. \\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 시작 토큰과 종료 토큰 추가\n",
    "sos_token = '\\t'\n",
    "eos_token = '\\n'\n",
    "lines.fra = lines.fra.apply(lambda x : '\\t '+ x + ' \\n')\n",
    "print('전체 샘플의 수 :',len(lines))\n",
    "lines.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4680a692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[19, 3, 8], [19, 3, 8], [19, 3, 8]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_tokenizer = Tokenizer(char_level=True) # 문자 단위로 Tokenizer를 생성함.\n",
    "eng_tokenizer.fit_on_texts(lines.eng) # 50000개의 행을 가진 eng의 각행에 토큰화를 수행함.\n",
    "input_text = eng_tokenizer.texts_to_sequences(lines.eng) # 단어를 숫자값 인덱스로 변환하여 저장함.\n",
    "input_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d6725dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 1, 19, 5, 1, 31, 1, 11],\n",
       " [10, 1, 15, 5, 12, 16, 29, 2, 14, 1, 11],\n",
       " [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fra_tokenizer = Tokenizer(char_level=True)\n",
    "fra_tokenizer.fit_on_texts(lines.fra)\n",
    "target_text = fra_tokenizer.texts_to_sequences(lines.fra)\n",
    "target_text[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c800ea6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 단어장의 크기 : 53\n",
      "프랑스어 단어장의 크기 : 73\n"
     ]
    }
   ],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) +1\n",
    "fra_vocab_size = len(fra_tokenizer.word_index) +1\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b6eae74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 시퀀스의 최대 길이 22\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "max_eng_seq_len = max([len(line) for line in input_text])\n",
    "max_fra_seq_len = max([len(line) for line in target_text])\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "73a325fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플의 수 : 50000\n",
      "영어 단어장의 크기 : 53\n",
      "프랑스어 단어장의 크기 : 73\n",
      "영어 시퀀스의 최대 길이 22\n",
      "프랑스어 시퀀스의 최대 길이 76\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플의 수 :',len(lines))\n",
    "print('영어 단어장의 크기 :', eng_vocab_size)\n",
    "print('프랑스어 단어장의 크기 :', fra_vocab_size)\n",
    "print('영어 시퀀스의 최대 길이', max_eng_seq_len)\n",
    "print('프랑스어 시퀀스의 최대 길이', max_fra_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74f9d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input = input_text\n",
    "\n",
    "# 종료 토큰 제거\n",
    "decoder_input = [[char for char in line if char != fra_tokenizer.word_index[eos_token] ] for line in target_text]\n",
    "\n",
    "# 시작 토큰 제거\n",
    "decoder_target = [[ char for char in line if char != fra_tokenizer.word_index[sos_token] ] for line in target_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ec53cdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[10, 1, 19, 5, 1, 31, 1], [10, 1, 15, 5, 12, 16, 29, 2, 14, 1], [10, 1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1]]\n",
      "[[1, 19, 5, 1, 31, 1, 11], [1, 15, 5, 12, 16, 29, 2, 14, 1, 11], [1, 2, 7, 1, 12, 9, 8, 4, 2, 1, 31, 1, 11]]\n"
     ]
    }
   ],
   "source": [
    "print(decoder_input[:3])\n",
    "print(decoder_target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0150639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 22)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = pad_sequences(encoder_input, maxlen = max_eng_seq_len, padding='post')\n",
    "decoder_input = pad_sequences(decoder_input, maxlen = max_fra_seq_len, padding='post')\n",
    "decoder_target = pad_sequences(decoder_target, maxlen = max_fra_seq_len, padding='post')\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68e85b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 데이터의 크기(shape) : (50000, 22, 53)\n",
      "프랑스어 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "encoder_input = to_categorical(encoder_input)\n",
    "decoder_input = to_categorical(decoder_input)\n",
    "decoder_target = to_categorical(decoder_target)\n",
    "print('영어 데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d4c6f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 학습데이터의 크기(shape) : (50000, 22, 53)\n",
      "프랑스어 학습 입력데이터의 크기(shape) : (50000, 76, 73)\n",
      "프랑스어 학습 출력데이터의 크기(shape) : (50000, 76, 73)\n"
     ]
    }
   ],
   "source": [
    "n_of_val = 3000\n",
    "\n",
    "encoder_input_train = encoder_input[:-n_of_val]\n",
    "decoder_input_train = decoder_input[:-n_of_val]\n",
    "decoder_target_train = decoder_target[:-n_of_val]\n",
    "\n",
    "encoder_input_test = encoder_input[-n_of_val:]\n",
    "decoder_input_test = decoder_input[-n_of_val:]\n",
    "decoder_target_test = decoder_target[-n_of_val:]\n",
    "\n",
    "print('영어 학습데이터의 크기(shape) :',np.shape(encoder_input))\n",
    "print('프랑스어 학습 입력데이터의 크기(shape) :',np.shape(decoder_input))\n",
    "print('프랑스어 학습 출력데이터의 크기(shape) :',np.shape(decoder_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7914b0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32215754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 설계\n",
    "\n",
    "# 입력 텐서 생성\n",
    "encoder_inputs = Input(shape=(None, eng_vocab_size))\n",
    "\n",
    "# hidden size가 256인 인코더의 LSTM셀 생성\n",
    "encoder_lstm = LSTM(units = 256, return_state = True)\n",
    "\n",
    "#디코더로 전달할 hidden state, cell state를 리턴\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# hidden state와 cell state를 다음 time step으로 전달하기 위해서 별도로 저장\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "068d98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 설계\n",
    "\n",
    "# 입력 텐서 생성\n",
    "decoder_inputs = Input(shape=(None, fra_vocab_size))\n",
    "\n",
    "# hidden size가 256인 인코더의 LSTM 셀 생성\n",
    "decoder_lstm = LSTM(units = 256, return_sequences = True, return_state=True)\n",
    "\n",
    "# decoder_outputs는 모든 time step의 hidden state\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state = encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77395845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더의 출력층 설계\n",
    "\n",
    "decoder_softmax_layer = Dense(fra_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_softmax_layer(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fff59e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 53)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 317440      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 73)     18761       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 674,121\n",
      "Trainable params: 674,121\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"categorical_crossentropy\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b87f06a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "368/368 [==============================] - 36s 24ms/step - loss: 0.8819 - val_loss: 0.7442\n",
      "Epoch 2/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.5360 - val_loss: 0.6203\n",
      "Epoch 3/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.4481 - val_loss: 0.5524\n",
      "Epoch 4/50\n",
      "368/368 [==============================] - 7s 19ms/step - loss: 0.3961 - val_loss: 0.4957\n",
      "Epoch 5/50\n",
      "368/368 [==============================] - 7s 19ms/step - loss: 0.3602 - val_loss: 0.4652\n",
      "Epoch 6/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3347 - val_loss: 0.4433\n",
      "Epoch 7/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.3147 - val_loss: 0.4281\n",
      "Epoch 8/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2987 - val_loss: 0.4020\n",
      "Epoch 9/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2853 - val_loss: 0.4037\n",
      "Epoch 10/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2740 - val_loss: 0.3922\n",
      "Epoch 11/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2645 - val_loss: 0.3819\n",
      "Epoch 12/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2559 - val_loss: 0.3736\n",
      "Epoch 13/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2484 - val_loss: 0.3722\n",
      "Epoch 14/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2415 - val_loss: 0.3683\n",
      "Epoch 15/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2353 - val_loss: 0.3628\n",
      "Epoch 16/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2297 - val_loss: 0.3585\n",
      "Epoch 17/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2244 - val_loss: 0.3641\n",
      "Epoch 18/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2196 - val_loss: 0.3598\n",
      "Epoch 19/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2150 - val_loss: 0.3524\n",
      "Epoch 20/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2108 - val_loss: 0.3559\n",
      "Epoch 21/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2067 - val_loss: 0.3559\n",
      "Epoch 22/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.2029 - val_loss: 0.3579\n",
      "Epoch 23/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1993 - val_loss: 0.3557\n",
      "Epoch 24/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1958 - val_loss: 0.3566\n",
      "Epoch 25/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1924 - val_loss: 0.3557\n",
      "Epoch 26/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1893 - val_loss: 0.3586\n",
      "Epoch 27/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1862 - val_loss: 0.3583\n",
      "Epoch 28/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1833 - val_loss: 0.3615\n",
      "Epoch 29/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1804 - val_loss: 0.3602\n",
      "Epoch 30/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1776 - val_loss: 0.3655\n",
      "Epoch 31/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1750 - val_loss: 0.3631\n",
      "Epoch 32/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1725 - val_loss: 0.3704\n",
      "Epoch 33/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1700 - val_loss: 0.3668\n",
      "Epoch 34/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1677 - val_loss: 0.3759\n",
      "Epoch 35/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1655 - val_loss: 0.3743\n",
      "Epoch 36/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1632 - val_loss: 0.3746\n",
      "Epoch 37/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1610 - val_loss: 0.3793\n",
      "Epoch 38/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1589 - val_loss: 0.3787\n",
      "Epoch 39/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1570 - val_loss: 0.3814\n",
      "Epoch 40/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1550 - val_loss: 0.3845\n",
      "Epoch 41/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1530 - val_loss: 0.3867\n",
      "Epoch 42/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1512 - val_loss: 0.3881\n",
      "Epoch 43/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1493 - val_loss: 0.3942\n",
      "Epoch 44/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1476 - val_loss: 0.3949\n",
      "Epoch 45/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1458 - val_loss: 0.3989\n",
      "Epoch 46/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1443 - val_loss: 0.3980\n",
      "Epoch 47/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1426 - val_loss: 0.4023\n",
      "Epoch 48/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1410 - val_loss: 0.4082\n",
      "Epoch 49/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1395 - val_loss: 0.4119\n",
      "Epoch 50/50\n",
      "368/368 [==============================] - 7s 18ms/step - loss: 0.1380 - val_loss: 0.4122\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff2c85de670>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=[encoder_input_train, decoder_input_train], y=decoder_target_train, \\\n",
    "         validation_data = ([encoder_input_test, decoder_input_test], decoder_target_test),\n",
    "         batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ae229b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 53)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 317440    \n",
      "=================================================================\n",
      "Total params: 317,440\n",
      "Trainable params: 317,440\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(inputs = encoder_inputs, outputs = encoder_states)\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0553b09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이전 time step의 hidden state를 저장하는 텐서\n",
    "decoder_state_input_h = Input(shape=(256,))\n",
    "\n",
    "# 이전 time step의 cell state를 저장하는 텐서\n",
    "decoder_state_input_c = Input(shape=(256,))\n",
    "\n",
    "# 이전 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# decoder_states_inputs를 현재 time step의 초기상태로 사용\n",
    "# 구체적인 동작 자체는 def decode_sequence()에 구현\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state = decoder_states_inputs)\n",
    "\n",
    "# 현재 time step의 hidden state와 cell state를 하나의 변수에 저장\n",
    "decoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b1a1968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 73)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  337920      input_2[0][0]                    \n",
      "                                                                 input_3[0][0]                    \n",
      "                                                                 input_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 337,920\n",
      "Trainable params: 337,920\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 디코더의 출력층을 재설계\n",
    "decoder_outpus = decoder_softmax_layer(decoder_outputs)\n",
    "decoder_model = Model(inputs=[decoder_inputs] + decoder_states_inputs, outputs=[decoder_outputs] + decoder_states)\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c0422b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng2idx = eng_tokenizer.word_index\n",
    "fra2idx = fra_tokenizer.word_index\n",
    "idx2eng = eng_tokenizer.index_word\n",
    "idx2fra = fra_tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b60a3cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # 입력으로부터 인코더의 상태를 얻음\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # <SOS>에 해당하는 원-핫 벡터 생성\n",
    "    target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "    target_seq[0, 0, fra2idx['\\t']] = 1.\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    \n",
    "    # stop_condition이 True가 될때까지 루프 반복\n",
    "    while not stop_condition:\n",
    "        # 이전 시점의 상태 states_value를 현 시점의 초기상태로 사용\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # 예측 결과를 문자로 변환\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = idx2fra[sampled_token_index]\n",
    "        \n",
    "        # 현재 시점의 예측 문자를 예측 문장에 추가\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        # <eos>에 도달하거나 최대 길이를 넘으면 중단\n",
    "        if (sampled_char == '\\n' or\n",
    "            len(decoded_sentence) > max_fra_seq_len):\n",
    "            stop_condition = True\n",
    "            \n",
    "        # 현재 시점의 예측결과를 다음 시점의 입력으로 사용하기 위해 저장\n",
    "        target_seq = np.zeros((1, 1, fra_vocab_size))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "        \n",
    "        # 현재 시점의 상태를 다음 시점의 상태로 사용하기 위해 저장\n",
    "        states_value = [h, c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d099d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for seq_index in [3, 50, 100, 300, 1001]: # 입력 문장의 인덱스\n",
    "    input_seq = encoder_input[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(35 * \"-\")\n",
    "    print('입력 문장:', lines.eng[seq_index])\n",
    "    print('정답 문장:', lines.fra[seq_index][1:len(lines.fra[seq_index])-1]) # '\\t'와 '\\n'을 빼고 출력\n",
    "    print('번역기가 번역한 문장:', decoded_sentence[:len(decoded_sentence)-1]) # '\\n'을 빼고 출력"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
