{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07e60120",
   "metadata": {},
   "source": [
    "## 1. 패키지 임포트 및 데이터 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83b395c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " ['', '', '[Spoken Intro:]', 'You ever want something ', \"that you know you shouldn't have \", \"The more you know you shouldn't have it, \", 'The more you want it ', 'And then one day you get it, ', \"It's so good too \", \"But it's just like my girl \", \"When she's around me \", 'I just feel so good, so good ', 'But right now I just feel cold, so cold ', 'Right down to my bones ', \"'Cause ooh... \", \"Ain't no sunshine when she's gone \", \"It's not warm when she's away \", \"Ain't no sunshine when she's gone \", \"And she's always gone too long \", 'Anytime she goes away ']\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import re\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path) # txt_file_path 경로에 있는 모든 파일명을 리스트 형식으로 txt_list에 할당\n",
    "\n",
    "raw_corpus = [] \n",
    "\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines() #  문자열을 한라인씩 분리하여 리스트로 반환\n",
    "        raw_corpus.extend(raw) \n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62100bb1",
   "metadata": {},
   "source": [
    "## 2. 데이터 전처리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5a12fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 소문자로 바꾸고, 양쪽 공백을 지웁니다\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 특수문자 양쪽에 공백을 넣고\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 여러개의 공백은 하나의 공백으로 바꿉니다\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # a-zA-Z?.!,¿가 아닌 모든 문자를 하나의 공백으로 바꿉니다\n",
    "    sentence = sentence.strip() # 양쪽 공백을 지웁니다\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 문장 시작에는 <start>, 끝에는 <end>를 추가합니다\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80cffee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> you ever want something <end>',\n",
       " '<start> that you know you shouldn t have <end>',\n",
       " '<start> the more you know you shouldn t have it , <end>',\n",
       " '<start> the more you want it <end>',\n",
       " '<start> and then one day you get it , <end>',\n",
       " '<start> it s so good too <end>',\n",
       " '<start> but it s just like my girl <end>',\n",
       " '<start> when she s around me <end>',\n",
       " '<start> i just feel so good , so good <end>',\n",
       " '<start> but right now i just feel cold , so cold <end>',\n",
       " '<start> right down to my bones <end>',\n",
       " '<start> cause ooh . . . <end>',\n",
       " '<start> ain t no sunshine when she s gone <end>',\n",
       " '<start> it s not warm when she s away <end>',\n",
       " '<start> ain t no sunshine when she s gone <end>',\n",
       " '<start> and she s always gone too long <end>',\n",
       " '<start> anytime she goes away <end>',\n",
       " '<start> wonder this time where she s gone <end>',\n",
       " '<start> wonder if she s gone to stay <end>',\n",
       " '<start> ain t no sunshine when she s gone <end>']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue # 공백인 문장은 건너뜀.\n",
    "    if sentence[-1] == \":\": continue # 문장의 끝이 : 인 문장은 건너뜁니다.\n",
    "    if sentence[-1] == \"]\": continue # 문장의 끝이 ] 인 문장은 건너뜁니다.                               \n",
    "        \n",
    "    # 앞서 구현한 preprocess_sentence() 함수를 이용하여 문장을 정제를 하고 담아주세요\n",
    "    preprocessed_sentence = preprocess_sentence( sentence )\n",
    "    corpus.append( preprocessed_sentence )\n",
    "    \n",
    "        \n",
    "# 정제된 결과를 20개만 확인해보죠\n",
    "corpus[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a0c571",
   "metadata": {},
   "source": [
    "## 3. 데이터를 토큰화하고, 텐서 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b38e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2   7 156 ...   0   0   0]\n",
      " [  2  17   7 ...   0   0   0]\n",
      " [  2   6  98 ...   0   0   0]\n",
      " ...\n",
      " [  2 310   1 ...   0   0   0]\n",
      " [  2 729   5 ...   0   0   0]\n",
      " [  2 729   5 ...   0   0   0]] <keras_preprocessing.text.Tokenizer object at 0x7f55906c8880>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words=12000,  # 12000단어를 기억할 수 있는 tokenizer\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\" # 12000단어에 포함되지 못한 단어는 '<unk>'로 바꿈.\n",
    "    )\n",
    "    tokenizer.fit_on_texts(corpus) # 문자 데이터를 입력받아 리스트의 형태로 변환\n",
    "\n",
    "    tensor = tokenizer.texts_to_sequences(corpus) # 텍스트 안의 단어들을 숫자의 시퀀스 형태로 변환\n",
    "       \n",
    "    # 문장 뒤에 패딩을 붙여 입력 데이터의 시퀀스 길이를 일정하게 맞춰줌.\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post') \n",
    "\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)\n",
    "print(tensor, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa18fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n",
      "11 : it\n",
      "12 : me\n",
      "13 : my\n",
      "14 : in\n",
      "15 : t\n",
      "16 : s\n",
      "17 : that\n",
      "18 : on\n",
      "19 : of\n",
      "20 : your\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word: # 단어의 인덱스와 인덱스에 해당하는 단어를 딕셔너리 형으로 반환함. (단어사전)\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 20: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9e01c",
   "metadata": {},
   "source": [
    "## 4. 소스문장과 타겟문장을 생성하고, 데이터셋 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1238e37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor에서 마지막 토큰을 잘라내서 소스 문장을 생성함.\n",
    "src_input = tensor[:, :-1]  \n",
    "\n",
    "# tensor에서 <start>를 잘라내서 타겟 문장을 생성함.\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "# 훈련 데이터와 평가 데이터 분리\n",
    "from  sklearn.model_selection  import  train_test_split\n",
    "enc_train, enc_val, dec_train, dec_val =  train_test_split( src_input, \n",
    "                                                            tgt_input,  \n",
    "                                                            test_size=0.2, \n",
    "                                                            random_state=10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb591f50",
   "metadata": {},
   "source": [
    "## 5. 언어 모델의 레이어 구성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dcbdaa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = tf.keras.layers.Embedding( vocab_size, embedding_size ) # 단어 사전의 인덱스값을 해당 인덱스번째의 워드벡터로 바꿔줌.\n",
    "        self.rnn_1 = tf.keras.layers.LSTM( hidden_size, return_sequences=True )  \n",
    "        self.rnn_2 = tf.keras.layers.LSTM( hidden_size, return_sequences=True )\n",
    "        self.linear = tf.keras.layers.Dense( vocab_size )\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "vocab_size = tokenizer.num_words + 1 # tokenizer가 구축한 단어사전 내 7000개와, 여기 포함되지 않은 0:<pad>를 포함하여 7001개     \n",
    "embedding_size = 256 # 워드 벡터의 차원수를 말하며, 단어가 추상적으로 표현될 수 있는 크기임.\n",
    "hidden_size = 1024 \n",
    "model = TextGenerator(vocab_size, embedding_size , hidden_size) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81664743",
   "metadata": {},
   "source": [
    "## 6. 언어 모델 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe57f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy( # 훈련 데이터의 라벨이 정수의 형태로 제공될 때 사용하는 손실함수\n",
    "    from_logits=True, # 모델에 의해 생성된 출력 값이 정규화되지 않았음을 손실 함수에 알려줌.\n",
    "    reduction='none')  # 각자 나오는 값의 반환을 원할 때 None을 사용함.\n",
    "\n",
    "model.compile(loss=loss, optimizer=optimizer) \n",
    "model.fit(enc_train, dec_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22d648e",
   "metadata": {},
   "source": [
    "## 7. 작문을 수행하는 함수 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bb83f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text( model, tokenizer, init_sentence=\"<start>\", max_len=20 ): \n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence]) \n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "        predict = model(test_tensor) \n",
    "        predict_word = tf.argmax( tf.nn.softmax( predict, axis=-1 ), axis=-1 )[:, -1]  \n",
    "          # 예측된 값 중 가장 높은 확률의 word index를 뽑아냄.\n",
    "\n",
    "        test_tensor = tf.concat([ test_tensor, tf.expand_dims( predict_word, axis=0 )], axis=-1 )\n",
    "          # 예측된 word index를 문장 뒤에 붙임.\n",
    "\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "         # 모델이 <end>를 예측했거나, max_len에 도달했다면 문장 생성을 마침.\n",
    "\n",
    "    generated = \"\"\n",
    "\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"    # word index를 단어로 하나씩 변환함.\n",
    "        return generated   # 최종적으로 모델이 생성한 문장을 반환함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6485c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_text 함수에 모델을 이용해서 ilove 로 시작되는 문장을 생성함.\n",
    "generate_text(model, tokenizer, init_sentence=\"<start> i love\", max_len=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b156ae6",
   "metadata": {},
   "source": [
    "## 8. 언어 모델 평가하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469ec017",
   "metadata": {},
   "outputs": [],
   "source": [
    "dec_pred = model.predict(enc_val)\n",
    "\n",
    "from  sklearn.metrics  import  classification_report\n",
    "print(classification_report( dec_val, dec_pred ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
